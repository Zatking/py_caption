import os
import json
from pathlib import Path
from PIL import Image

import torch
from torch.utils.data import Dataset, DataLoader
from transformers import BlipProcessor, BlipForConditionalGeneration
from torch.optim import AdamW

# =======================
# STEP 1: Prepare Dataset
# =======================
def prepare_dataset(image_dir, caption_json, output_json):
    with open(caption_json, "r", encoding="utf-8") as f:
        all_captions = json.load(f)

    if not all_captions:
        print("‚ö†Ô∏è File caption tr·ªëng ho·∫∑c kh√¥ng h·ª£p l·ªá!")
        return []

    dataset = []
    image_dir = Path(image_dir)

    for image_path in image_dir.rglob("*.[jp][pn]g"):  # jpg, png
        rel_path = image_path.relative_to(image_dir).as_posix()
        disease_label = image_path.parent.name

        caption = all_captions.get(rel_path)
        if caption:
            dataset.append({
                "image": str(image_path),
                "caption": caption[0] if isinstance(caption, list) else caption,
                "label": disease_label
            })
        else:
            print(f"‚ö†Ô∏è Kh√¥ng t√¨m th·∫•y caption cho {rel_path}")

    if not dataset:
        print("‚ö†Ô∏è Kh√¥ng c√≥ d·ªØ li·ªáu h·ª£p l·ªá ƒë·ªÉ hu·∫•n luy·ªán!")
        return []

    with open(output_json, "w", encoding="utf-8") as f:
        json.dump(dataset, f, ensure_ascii=False, indent=2)

    print(f"‚úÖ ƒê√£ l∆∞u dataset v√†o {output_json} v·ªõi {len(dataset)} m·∫´u.")
    return dataset

# =======================
# STEP 2: Dataset & Preprocess
# =======================
class CaptionDataset(Dataset):
    def __init__(self, data, processor):
        self.data = data
        self.processor = processor

    def __len__(self):
        return len(self.data)

    def __getitem__(self, idx):
        item = self.data[idx]
        image = Image.open(item["image"]).convert("RGB")
        inputs = self.processor(images=image, text=item["caption"], return_tensors="pt", padding="max_length", truncation=True)
        image_inputs = self.processor(images=image, return_tensors="pt")
        text_inputs = self.processor(text=item["caption"], return_tensors="pt", padding="max_length", truncation=True)

        return {
        "pixel_values": image_inputs["pixel_values"].squeeze(0),
        "input_ids": text_inputs["input_ids"].squeeze(0),
        "attention_mask": text_inputs["attention_mask"].squeeze(0),
        "labels": text_inputs["input_ids"].squeeze(0)  # d√πng ch√≠nh input l√†m label
        }

# =======================
# STEP 3: Training
# =======================
def train(model, processor, dataset, epochs=5, batch_size=4, lr=2e-5):
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    
    # Chuy·ªÉn m√¥ h√¨nh l√™n GPU    
    model.to(device)

    dataloader = DataLoader(CaptionDataset(dataset, processor), batch_size=batch_size, shuffle=True)
    optimizer = AdamW(model.parameters(), lr=lr,weight_decay=0.01, eps=1e-8)
    model.train()

    for epoch in range(epochs):
        total_loss = 0
        for batch in dataloader:
            # Chuy·ªÉn batch d·ªØ li·ªáu l√™n GPU
            batch = {k: v.to(device) for k, v in batch.items()}

            outputs = model(
                        pixel_values=batch['pixel_values'],
                        input_ids=batch['input_ids'],
                        attention_mask=batch['attention_mask'],
                        labels=batch['labels']
                        
                            )
            loss = outputs.loss
            loss.backward()
            optimizer.step()
            optimizer.zero_grad()
            total_loss += loss.item()
        print(f"üìö Epoch {epoch+1}: Loss = {total_loss / len(dataloader):.4f}")

# =======================
# STEP 4: Inference
# =======================
def predict(image_path, model, processor):
    model.eval()
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    
    # Chuy·ªÉn m√¥ h√¨nh l√™n GPU
    model.to(device)
    
    image = Image.open(image_path).convert("RGB")
    inputs = processor(image, return_tensors="pt").to(device)  # Chuy·ªÉn input l√™n GPU
    
    out = model.generate(**inputs)
    caption = processor.decode(out[0], skip_special_tokens=True)
    print(f"üñºÔ∏è Image: {image_path}")
    print(f"üìù Predicted Caption: {caption}")

# =======================
# MAIN
# =======================
if __name__ == "__main__":
    image_dir = "app/static/images/test_set"
    caption_json = "app/static/caption.json"
    output_json = "app/static/prepared_dataset.json"

    # B∆∞·ªõc 1: Chu·∫©n b·ªã d·ªØ li·ªáu
    dataset = prepare_dataset(image_dir, caption_json, output_json)
    if not dataset:
        exit()

    # B∆∞·ªõc 2: Load BLIP model + processor
    print("üöÄ ƒêang load m√¥ h√¨nh BLIP...")
    processor = BlipProcessor.from_pretrained("Salesforce/blip-image-captioning-large")
    model = BlipForConditionalGeneration.from_pretrained("Salesforce/blip-image-captioning-large")

    # B∆∞·ªõc 3: Fine-tune nhanh
    print("üß† B·∫Øt ƒë·∫ßu fine-tune m√¥ h√¨nh...")
    train(model, processor, dataset, epochs=4)  # b·∫°n c√≥ th·ªÉ tƒÉng s·ªë epoch

    # B∆∞·ªõc 4: D·ª± ƒëo√°n th·ª≠ caption ·∫£nh ƒë·∫ßu ti√™n
    print("\nüîç D·ª± ƒëo√°n caption ·∫£nh ƒë·∫ßu ti√™n:")
    predict(dataset[0]["image"], model, processor)
